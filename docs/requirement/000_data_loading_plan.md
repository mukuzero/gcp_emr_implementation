# Data Loading Requirement and Implementation Plan

## Problem Statement
The current project has a database setup script (`scripts/setup_db.sh`) that defines the schema and a data generator script (`scripts/generate_hospital_data.py`) that produces synthetic data in CSV format. However, there is no automated mechanism to load this generated data into the database tables.

## Gap Analysis

### 1. Schema vs. Data Alignment
I have compared the DDL in `scripts/ddl.sql` with the data generated by `scripts/generate_hospital_data.py`.
- **Hospitals**: Columns match perfectly.
- **Patients**: Columns match perfectly.
- **Departments**: Columns match perfectly.
- **Providers**: Columns match perfectly.
- **Encounters**: Columns match perfectly.
- **Transactions**: Columns match perfectly.

**Conclusion**: No changes are needed to the schema or the data generator's output structure.

### 2. Missing Loading Mechanism
- `setup_db.sh` is responsible for DDL execution only.
- `generate_hospital_data.py` runs locally and outputs CSV files to the current directory.
- There is no script to bridge the gap: taking the CSVs and inserting them into the Cloud SQL instance.

### 3. Execution Environment
- The loading process requires access to the Cloud SQL instance, likely via the Cloud SQL Proxy, similar to `setup_db.sh`.
- It requires authentication credentials (`DB_USER`, `DB_PASSWORD`, etc.).

## Implementation Plan

### 1. Create `scripts/load_data.sh`
This script will serve as the orchestrator for data loading. It will perform the following steps:

1.  **Environment Setup**: Validate required environment variables (`REGION`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, `DB_INSTANCE_NAME_PREFIX`).
2.  **Proxy Setup**: Locate the Cloud SQL instance and start the Cloud SQL Proxy (reusing logic from `setup_db.sh`).
3.  **Data Generation**: Execute `python3 scripts/generate_hospital_data.py` to generate fresh CSV files.
4.  **Data Loading**:
    - Use `psql` with the `\copy` command to efficiently bulk load data from the CSV files into the corresponding tables.
    - The `\copy` command runs on the client side and streams data to the server, which is ideal for this setup.
    - **Load Order**: Hospitals -> Departments -> Providers -> Patients -> Encounters -> Transactions (to respect logical dependencies, even if strict FKs aren't enforced).
5.  **Cleanup**: Remove the generated CSV files after successful loading.
6.  **Proxy Teardown**: Stop the Cloud SQL Proxy.

### 2. Verification
- **Manual Verification**: Connect to the database and run `SELECT COUNT(*) FROM <table>` for each table to verify data presence.
- **Automated Check**: The script can include a final step that queries the row counts and prints them to the console.

## Technical Details

### `psql \copy` Command
The core loading command will look like this:
```bash
psql -h 127.0.0.1 -p 5432 -U "$DB_USER" -d "$DB_NAME" -c "\copy hospitals(hospitalID, Name, Address, PhoneNumber, created_at, updated_at, deleted_at) FROM 'hospitals.csv' DELIMITER ',' CSV HEADER;"
```

### File Management
The generator currently outputs to the current working directory. The script should handle this by either moving files to a temp dir or running the generator in a temp dir to avoid clutter.

## Next Steps
1.  Approve this plan.
2.  Implement `scripts/load_data.sh`.
3.  Test the loading process.
